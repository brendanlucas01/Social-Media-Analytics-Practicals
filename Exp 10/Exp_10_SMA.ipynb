{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkZHnABLjwEU"
   },
   "source": [
    "# Steps of LDA (Latent Dirichlet Allocation):\n",
    "1 .Choose the number of topics (k) you want to extract from the corpus.\n",
    "\n",
    "2.Preprocess the reviews corpus by removing stop words, punctuations, and converting words to their root forms using stemming or\n",
    "lemmatization.\n",
    "\n",
    "3.Create a vocabulary list of all unique words in the corpus.\n",
    "\n",
    "4.Convert each review in the corpus into a bag-of-words representation, where each word is represented by its index in the vocabulary list and\n",
    "the count of that word in the review.\n",
    "\n",
    "5.1nitialize the model by randomly assigning each word in each review to one of the k topics.\n",
    "\n",
    "6.For each review 'r' in the corpus, iterate through each word w in the review and calculate the probability distribution over the k topics, given the current assignments of all other words in the document to their topics and the current topic-word distribution.\n",
    "\n",
    "7.Sample a new topic assignment for word w based on the probability distribution calculated in step 6.\n",
    "\n",
    "8.Repeat steps 6 and 7 for all reviewss in the corpus until convergence is achieved.\n",
    "\n",
    "9.0utput the topic-word distribution and document-topic distribution as the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cs6cUU8ZhfVF",
    "outputId": "a148b12e-6fa2-4409-ea61-66c2205e23ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJZpZFEIhuuD",
    "outputId": "31daa24a-c764-4e2d-b925-1b4ee578c861"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Test\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.2-cp310-cp310-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 5.2 MB/s eta 0:00:00\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-win_amd64.whl (480 kB)\n",
      "     -------------------------------------- 480.9/480.9 kB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-win_amd64.whl (18 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-win_amd64.whl (94 kB)\n",
      "     ---------------------------------------- 94.7/94.7 kB 5.6 MB/s eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (22.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.6/181.6 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.9/48.9 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 spacy-3.5.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 wasabi-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpra\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as pit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIjw4fxgh4B3"
   },
   "outputs": [],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# NLJK Stop words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "id": "bx1sdi1WiitP",
    "outputId": "f4e5166d-3949-4e73-93d9-65ee7fd86123"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('McDonald.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgBy6cbRk-Eu"
   },
   "outputs": [],
   "source": [
    "df = df[[\"wiI7pd\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IA0Zmdw9lSja",
    "outputId": "8efce52a-4995-46a3-dcea-8e6682692164"
   },
   "outputs": [],
   "source": [
    "df.rename(columns = {'wiI7pd':'reviews'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2h4B_YAlmvo",
    "outputId": "e3845932-e255-4508-dad4-0b8889cbbe8c"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgGBer2RkPA2"
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "  for sent in sentences:\n",
    "    sent=re.sub('\\s+',' ',sent)\n",
    "    sent=re.sub(\"\\'\",\"\",sent)\n",
    "    sent = gensim.utils.simple_preprocess(str(sent),deacc=True)\n",
    "    yield(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebto5Pcyk8wW",
    "outputId": "771a4381-8520-410c-a683-1baeaaf8b7d6"
   },
   "outputs": [],
   "source": [
    "# convert to list\n",
    "All_reviews  = df.reviews.values.tolist()\n",
    "reviews_words=list(sent_to_words(All_reviews))\n",
    "print(reviews_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7EbfDE_mOgu",
    "outputId": "4785aaaa-0cf8-4c59-f3fc-9a1e667d6bf2"
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5qZlTx6mQzk"
   },
   "outputs": [],
   "source": [
    "bigram  = gensim.models.Phrases(reviews_words,min_count=5, threshold=10)\n",
    "trigram = gensim.models.Phrases(bigram[reviews_words],threshold = 10)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod  = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpEMfd1Gm_Vi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "us_hI15HnbLX"
   },
   "outputs": [],
   "source": [
    "def process_words(texts,stop_words = stop_words, allowed_postages = [\"NOUN\",'ADJ','VERB','ADV']):\n",
    "  texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "  texts = [bigram_mod[doc] for doc in texts]\n",
    "  texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "  texts_out = []\n",
    "  nlp = spacy.load('en_core_web_sm',disable=['parser','ner'])\n",
    "  for sent in texts:\n",
    "    doc = nlp(\" \".join(sent))\n",
    "    texts_out.append([token.lemma_ for token in doc]) \n",
    "  texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]\n",
    "  return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHZSVd2lpfbP"
   },
   "outputs": [],
   "source": [
    "data_final=process_words(reviews_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnymb1G_qFAj",
    "outputId": "fddb57d5-4687-41fd-ca2d-08a179a2d981"
   },
   "outputs": [],
   "source": [
    "data_final[:3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vD5-cdPV1HWx",
    "outputId": "5bd2669e-eb4b-43cf-b4e1-bf2c8350fd80"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WIAIhffzk_2"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "# Create Dictionary\n",
    "id2word = Dictionary(data_final)\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus=[id2word.doc2bow(text) for text in data_final]\n",
    "#Build LDA model\n",
    "\n",
    "lda_model=gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                        id2word=id2word,\n",
    "                                        num_topics=7,\n",
    "                                        random_state=100,\n",
    "                                        update_every=1,\n",
    "                                        chunksize=10,\n",
    "                                        passes=10,\n",
    "                                        alpha=\"symmetric\" ,\n",
    "                                        iterations=100,\n",
    "                                        per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgZFuJza2aSY",
    "outputId": "a13e4082-315c-40cf-dfc7-2ceb9d08208e"
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIh1kLrg2f0A"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03g4MAp23AkD"
   },
   "outputs": [],
   "source": [
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "cloud = WordCloud(stopwords = stop_words,\n",
    "                  background_color=\"white\" ,\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap=\"tab10\",\n",
    "                  color_func= lambda *args,**kwargs:cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics =lda_model.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1VFlyJA6f2D",
    "outputId": "9e4f1f85-34a9-4189-e27c-b6b514632dfd"
   },
   "outputs": [],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "olL6_q0w4KKz",
    "outputId": "2a1ef642-41bd-413e-987a-1377d8ae06e1"
   },
   "outputs": [],
   "source": [
    "fig, axes = pit.subplots(3, 2,figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "  fig. add_subplot(ax)\n",
    "  topic_words = dict(topics[i][1])\n",
    "  cloud.generate_from_frequencies(topic_words,max_font_size=300)\n",
    "  plt.gca().imshow(cloud)\n",
    "  plt.gca().set_title('Topic '+str(i),fontdict=dict(size=16))\n",
    "  plt.gca().axis('off')\n",
    "\n",
    "pit.subplots_adjust(wspace=0,hspace=0)\n",
    "plt.axis( 'off' )\n",
    "plt.margins(x=0, y=0)\n",
    "pit.tight_layout()\n",
    "pit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "id": "vAROajKU610U",
    "outputId": "a7171362-aeab-4eae-e8fb-938ecd985359"
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for i in range(7):\n",
    "  words = lda_model.show_topic(i,topn=20)\n",
    "  word_dict['Topic # '+'{:02d}'.format(i)] = [i[0] for i in words]\n",
    "\n",
    "pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nioR-ce7kkv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
